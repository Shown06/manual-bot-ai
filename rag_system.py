"""
RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆRetrieval-Augmented Generationï¼‰
Chromaã‚’ä½¿ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«DB + OpenAI Embeddings
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
import openai
import os
from dotenv import load_dotenv
from typing import List, Dict
import sqlite3

load_dotenv()

class RAGSystem:
    def __init__(self, user_id, persist_directory="./chroma_db"):
        """
        RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼IDï¼ˆãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œï¼‰
            persist_directory: Chroma DBã®ä¿å­˜å…ˆ
        """
        self.user_id = user_id
        self.persist_directory = f"{persist_directory}/user_{user_id}"
        
        # OpenAI Embeddings
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.getenv("OPENAI_API_KEY_DOCLING") or os.getenv("OPENAI_API_KEY")
        )
        
        # Chroma ãƒ™ã‚¯ãƒˆãƒ«DB
        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ã€", " "]
        )
        
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY_DOCLING") or os.getenv("OPENAI_API_KEY")
        )
    
    def add_document(self, markdown_text: str, metadata: Dict):
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’RAGã‚·ã‚¹ãƒ†ãƒ ã«è¿½åŠ 
        
        Args:
            markdown_text: Markdownå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆ
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆfilename, file_id, etc.ï¼‰
        
        Returns:
            int: è¿½åŠ ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°
        """
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = self.text_splitter.split_text(markdown_text)
        
        # Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        documents = [
            Document(
                page_content=chunk,
                metadata={
                    **metadata,
                    "user_id": self.user_id,
                    "chunk_index": i
                }
            )
            for i, chunk in enumerate(chunks)
        ]
        
        # ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜
        self.vectorstore.add_documents(documents)
        self.vectorstore.persist()
        
        return len(chunks)
    
    def search(self, query: str, top_k: int = 5) -> List[Document]:
        """
        é¡ä¼¼æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: å–å¾—ã™ã‚‹çµæœæ•°
        
        Returns:
            List[Document]: é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        results = self.vectorstore.similarity_search(
            query,
            k=top_k,
            filter={"user_id": self.user_id}
        )
        
        return results
    
    def qa(self, question: str, top_k: int = 5) -> Dict:
        """
        è³ªç–‘å¿œç­”
        
        Args:
            question: è³ªå•
            top_k: æ¤œç´¢ã™ã‚‹é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        
        Returns:
            dict: {
                'answer': å›ç­”,
                'sources': ã‚½ãƒ¼ã‚¹æƒ…å ±,
                'cost': ã‚³ã‚¹ãƒˆ
            }
        """
        # é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢
        docs = self.search(question, top_k=top_k)
        
        if not docs:
            return {
                "answer": "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                "sources": [],
                "cost": 0
            }
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        context = "\n\n".join([
            f"[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i+1}]\n"
            f"ãƒ•ã‚¡ã‚¤ãƒ«å: {doc.metadata.get('filename', 'ä¸æ˜')}\n"
            f"å†…å®¹:\n{doc.page_content}"
            for i, doc in enumerate(docs)
        ])
        
        # GPT-4oã§å›ç­”ç”Ÿæˆ
        prompt = f"""ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
{context}

ã€è³ªå•ã€‘
{question}

ã€å›ç­”å½¢å¼ã€‘
1. è³ªå•ã«å¯¾ã™ã‚‹æ˜ç¢ºãªå›ç­”
2. å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå
3. è©²å½“ç®‡æ‰€ã®å¼•ç”¨ï¼ˆç°¡æ½”ã«ï¼‰

å›ç­”ã¯ç°¡æ½”ã«ã€æ­£ç¢ºã«ã€‚"""
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024
        )
        
        answer = response.choices[0].message.content
        
        # ã‚³ã‚¹ãƒˆè¨ˆç®—
        cost = (response.usage.prompt_tokens / 1_000_000 * 2.5) + \
               (response.usage.completion_tokens / 1_000_000 * 10)
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±
        sources = [
            {
                "filename": doc.metadata.get('filename', 'ä¸æ˜'),
                "chunk": doc.page_content[:100] + "..."
            }
            for doc in docs
        ]
        
        return {
            "answer": answer,
            "sources": sources,
            "cost": round(cost, 4)
        }
    
    def get_all_documents(self) -> List[str]:
        """
        ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
        """
        # Chromaã‹ã‚‰å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        all_docs = self.vectorstore.get()
        
        if not all_docs or 'metadatas' not in all_docs:
            return []
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
        filenames = set()
        for metadata in all_docs['metadatas']:
            if metadata.get('user_id') == self.user_id:
                filename = metadata.get('filename')
                if filename:
                    filenames.add(filename)
        
        return sorted(list(filenames))
    
    def delete_document(self, filename: str):
        """
        ç‰¹å®šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤
        
        Args:
            filename: å‰Šé™¤ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        # Chromaã‹ã‚‰è©²å½“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤
        all_docs = self.vectorstore.get()
        
        if not all_docs or 'ids' not in all_docs:
            return
        
        ids_to_delete = []
        for i, metadata in enumerate(all_docs['metadatas']):
            if metadata.get('user_id') == self.user_id and metadata.get('filename') == filename:
                ids_to_delete.append(all_docs['ids'][i])
        
        if ids_to_delete:
            self.vectorstore.delete(ids=ids_to_delete)
            self.vectorstore.persist()

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    rag = RAGSystem(user_id=1)
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ 
    markdown = "# ãƒ†ã‚¹ãƒˆæ–‡æ›¸\n\nå–¶æ¥­æ™‚é–“: 9:00-18:00"
    chunks = rag.add_document(
        markdown_text=markdown,
        metadata={"filename": "test.pdf", "file_id": 1}
    )
    print(f"âœ… {chunks}ãƒãƒ£ãƒ³ã‚¯è¿½åŠ ")
    
    # è³ªç–‘å¿œç­”
    result = rag.qa("å–¶æ¥­æ™‚é–“ã¯ï¼Ÿ")
    print(f"ğŸ’¬ å›ç­”: {result['answer']}")
    print(f"ğŸ’° ã‚³ã‚¹ãƒˆ: ${result['cost']}")


